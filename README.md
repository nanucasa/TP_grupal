# TP Grupal â€“ Telco Churn

- Proyecto MLOps de PredicciÃ³n de Churn
- Proyecto ISTEA | Materia: Laboratorio de MinerÃ­a de Datos

# esto es una nota para comprobar que el documento desde el branch del cliente ha sido subido satisfactoriamente por el PR

## ğŸ“‹ DescripciÃ³n del Proyecto

- Pipeline reproducible de Machine Learning para predecir la rotaciÃ³n de clientes (churn) en una empresa de telefonÃ­a, aplicando buenas prÃ¡cticas de MLOps con versionado de datos, tracking de experimentos y orquestaciÃ³n automatizada de assets.

**Contexto:**
- El objetivo es identificar quÃ© clientes tienen mayor probabilidad de darse de baja, utilizando informaciÃ³n de facturaciÃ³n, tipo de contrato y otros datos relacionados con el servicio.

**El proyecto integra:**
- 1- CÃ³digo versionado con Git/GitHub.
- 2- Datos y pipeline versionados con DVC.
- 3- Experimentos y modelos registrados en MLflow (remoto en DagsHub).
- 4- VisualizaciÃ³n y automatizaciÃ³n de selecciÃ³n de modelo campeÃ³n con Dagster.

## ğŸ¯ Objetivos
Predecir la baja de clientes de una telco (churn) usando modelos de Machine Learning y armar un flujo reproducible de:

- Construir un pipeline de ML completamente reproducible.
- Versionado de datos y modelos en **DagsHub** (DVC + MLflow Registry)
- PreparaciÃ³n de datos con **DVC**  
- Trackear experimentos y modelos con MLflow (DagsHub).
- OrquestaciÃ³n y elecciÃ³n de modelo campeÃ³n con **Dagster**  
- **CI** y **CD** en **GitHub Actions**  
- Seleccionar de forma sistemÃ¡tica un modelo campeÃ³n segÃºn F1.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- Python 3.10+ â€“ Lenguaje principal  
- DVC 3.63+ â€“ Versionado de datos y modelos  
- Git/GitHub â€“ Control de versiones  
- DagsHub â€“ Hosting remoto (DVC + MLflow)  
- MLflow 2.22+ â€“ Tracking y registro de modelos (remoto en DagsHub)  
- Dagster 1.12+ â€“ OrquestaciÃ³n y monitoreo de assets  
- dagster-webserver / dagster-daemon â€“ UI y demonios de orquestaciÃ³n  
- scikit-learn 1.5+ â€“ Modelado  
- Pandas / NumPy â€“ ManipulaciÃ³n de datos  
- Matplotlib â€“ GrÃ¡ficos de performance  

## ğŸ“Š Dataset

- Nombre: telco_churn.csv
- UbicaciÃ³n: data/raw/telco_churn.csv (trackeado por DVC)
- Target: churn (1 = se da de baja, 0 = permanece)
- Contenido: informaciÃ³n demogrÃ¡fica, de facturaciÃ³n y del tipo de contrato del cliente.
- Ejemplos de variables:
- customer_id: identificador Ãºnico
- tenure_months: tiempo como cliente
- monthly_charges: cargos mensuales
- total_charges: cargos acumulados
- contract_type: tipo de contrato
- churn: variable objetivo

## âš™ï¸ Requisitos Previos

- Python 3.10 o superior
- Conda / Anaconda
- Git
- DVC
- Cuenta en DagsHub
- Repositorios:
- GitHub: https://github.com/nanucasa/TP_grupal
- DagsHub: https://dagshub.com/nanucasa/TP_grupal

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n (guÃ­a paso a paso)

**Clonar el repositorio**
- git clone https://github.com/nanucasa/TP_grupal.git
- cd TP_grupal

**Crear y activar el entorno conda**
- conda create -n tp_grupal python=3.10 -y
- conda activate tp_grupal

**Instalar dependencias del proyecto**
- pip install -r requirements.txt

**Configurar el remoto de DVC (DagsHub) con credenciales personales**
- dvc remote modify origin --local auth basic
- dvc remote modify origin --local user TU_USUARIO_DAGSHUB
- dvc remote modify origin --local password TU_TOKEN_DAGSHUB

**Sincronizar datos versionados desde DagsHub**
- dvc pull

**Ejecutar la preparaciÃ³n de datos con DVC (solo data prep)**  
- `dvc repro data_prep`

Esto:
- Toma `data/raw/telco_churn.csv`.
- Genera `data/processed/train.csv`, `data/processed/valid.csv`,
  `data/processed/test.csv` y `data/processed/features.json`.

**Entrenar el modelo y loguear en MLflow (local + remoto)**  
- `python scripts/base_scripts_runs.py`

Esto:
- Usa los CSV de `data/processed/`.
- Entrena el modelo XGBoost (`TelcoChurn_XGB`).
- Registra un run en el experimento `telco_churn_tune_xgb`.
- Registra/actualiza el modelo `TelcoChurn_XGB` en el Model Registry remoto de DagsHub
  y gestiona el alias `champion`.

**Ver resultados en DagsHub / MLflow (tracking remoto)**
- Ir al repo en DagsHub.
- Abrir la pestaÃ±a â€œExperimentsâ€ (es la UI de MLflow remoto).
- Seleccionar el experimento telco_churn_tune_xgb para ver runs, mÃ©tricas y modelos registrados.

**(Opcional pero recomendado)** 
- Levantar Dagster para monitoreo y automatizaciÃ³n
- cd tp_grupal_dagster
- dagster dev

- Luego abrir en el navegador: http://127.0.0.1:3000
Desde allÃ­ se visualizan los assets, el sensor champion_sensor y el modelo campeÃ³n actualizado.

#### NOTA: El MLflow local (file:mlruns) puede existir, pero la fuente de verdad del proyecto y de Dagster es SIEMPRE el MLflow remoto de DagsHub.

## ğŸ“ Estructura del Proyecto

- TP_grupal/
- â”œâ”€â”€ data/
- â”‚ â”œâ”€â”€ raw/ # Datos originales (DVC)
- â”‚ â””â”€â”€ processed/ # Datos limpios (DVC)
- â”œâ”€â”€ src/
- â”‚ â”œâ”€â”€ data_prep.py # Limpieza y split de datos
- â”‚ â””â”€â”€ train.py # Entrenamiento + MLflow logging
- â”œâ”€â”€ models/ # Modelos entrenados
- â”‚ â””â”€â”€ model.joblib
- â”œâ”€â”€ tp_grupal_dagster/
- â”‚ â””â”€â”€ tp_grupal_dagster/
- â”‚ â”œâ”€â”€ assets.py # Assets Dagster
- â”‚ â”œâ”€â”€ definitions.py # Definitions Dagster
- â”‚ â””â”€â”€ init.py
- â”œâ”€â”€ artifacts/
- â”‚ â””â”€â”€ champion_metadata.json # InformaciÃ³n del modelo campeÃ³n
- â”œâ”€â”€ reports/ # GrÃ¡ficos (ROC, PR)
- â”œâ”€â”€ params.yaml # ParÃ¡metros del modelo
- â”œâ”€â”€ dvc.yaml # DefiniciÃ³n del pipeline
- â”œâ”€â”€ dvc.yaml
- â”œâ”€â”€ dvc.lock
- â”œâ”€â”€ params.yaml
- â”œâ”€â”€ requirements.txt
- â””â”€â”€ .github/
-     â””â”€â”€ workflows/
-        â”œâ”€â”€ ci.yml
-        â””â”€â”€ cd_retrain.yml
- â””â”€â”€ README.md

## ğŸ”„ Pipeline de Trabajo (DVC)

### Stage 1 â€“ `data_prep` (DVC)

**Script:** `src/data_prep.py`

**Funciones:**
- Carga del dataset crudo.
- Limpieza / preprocesamiento.
- Split en train / valid / test.
- GeneraciÃ³n del archivo `features.json` con la lista de features.

**Entradas:**
- `data/raw/telco_churn.csv`
- `params.yaml`

**Salidas:**
- `data/processed/train.csv`
- `data/processed/valid.csv`
- `data/processed/test.csv`
- `data/processed/features.json`

**Este stage se ejecuta automÃ¡ticamente cuando se corre:**
- `dvc repro data_prep`
- o simplemente `dvc repro` (es el Ãºnico stage actual del pipeline).

### Stage 2 â€“ Entrenamiento y logging (fuera de DVC)

**Script:** `scripts/base_scripts_runs.py`

**Funciones:**
- Carga los datos de `data/processed/`.
- Entrena el modelo XGBoost (`TelcoChurn_XGB`).
- Calcula mÃ©tricas (F1, accuracy, precision, recall, etc.).
- Loguea resultados en MLflow local y remoto (DagsHub).
- Registra y actualiza el modelo `TelcoChurn_XGB` en el Model Registry,
  incluyendo la administraciÃ³n del alias `champion`.

**Comando:**
- python scripts/base_scripts_runs.py

## ğŸ“š Comandos Ãºtiles
# Ver estado del pipeline
- dvc status

# Reproducir solo la preparaciÃ³n de datos
- dvc repro data_prep

# Ver el grafo del pipeline
- dvc dag

## ğŸ“š GuÃ­a rÃ¡pida paso a paso (resumen)

**1- Preparar entorno**
- Git clone del repositorio.
- Crear y activar entorno conda tp_grupal.
- Instalar requirements.txt.

**2- Sincronizar datos**
- Configurar remoto DVC con usuario/token de DagsHub.
- Ejecutar dvc pull.

**3- Ejecutar el pipeline completo**
- Ejecutar `dvc repro data_prep`.

**4- Entrenar el modelo y registrar experimentos**  
- Ejecutar `python scripts/base_scripts_runs.py`.

**5- Verificar que se generen:**  
- `data/processed/train.csv`, `valid.csv`, `test.csv`, `features.json`.  
- Nuevos runs en el experimento `telco_churn_tune_xgb` en la pestaÃ±a **Experiments** de DagsHub.  
- Nuevas versiones del modelo `TelcoChurn_XGB` en la pestaÃ±a **Models** de DagsHub, con el alias `champion` actualizado.

**6- Monitoreo y automatizaciÃ³n con Dagster**  
- Desde la raÃ­z del proyecto de orquestaciÃ³n:  
  - `cd tp_grupal_dagster`  
  - `dagster dev`  

- Abrir `http://127.0.0.1:3000` y revisar:
  - Assets de champion.  
  - Sensor `champion_sensor` (detecta nuevo campeÃ³n).  

**7- Confirmar modelo campeÃ³n:**  
- Revisar `tp_grupal_dagster/artifacts/champion_metadata.json` (actualizado por Dagster a partir del MLflow remoto de DagsHub).  
- Verificar en el Model Registry de DagsHub que la versiÃ³n correspondiente de 
  `TelcoChurn_XGB` tenga el alias `champion`.

**8- Confirmar modelo campeÃ³n:**
- Revisar artifacts/champion_metadata.json (actualizado por Dagster; siempre refleja el MLflow remoto de DagsHub).
- Verificar que en MLflow Model Registry (en la UI de DagsHub) el modelo tenga alias champion.

## ğŸ§© Monitoreo y AutomatizaciÃ³n con Dagster

- Dagster monitorea automÃ¡ticamente el experimento telco_churn_tune_xgb en el MLflow remoto de DagsHub y selecciona el mejor run segÃºn la mÃ©trica F1.

### Cuando detecta un nuevo campeÃ³n:

**Materializa los assets:**
- select_champion_from_mlflow
- persist_champion_json
- set_mlflow_champion_alias

- Actualiza el archivo local artifacts/champion_metadata.json con la informaciÃ³n del nuevo campeÃ³n (lectura siempre desde DagsHub).
- Asigna el alias champion al modelo correspondiente en el Model Registry de MLflow (tambiÃ©n en DagsHub).
- El sensor champion_sensor solo se dispara si existe un nuevo run con F1 superior al actual, evitando ejecuciones en bucle innecesarias.

### Â¿En quÃ© MLflow se ve el champion y cÃ³mo llegar?

- Ir a: **https://dagshub.com/nanucasa/TP_grupal**
- Abrir la pestaÃ±a â€œExperimentsâ€ (UI de MLflow remoto).
- Seleccionar el experimento telco_churn_tune_xgb.
- Ordenar la columna f1 de mayor a menor.
- El primer run de la tabla es el campeÃ³n; su run_id coincide con el que aparece en artifacts/champion_metadata.json.

**Para ver el modelo en el registry:**
- Desde la misma UI de MLflow en DagsHub, ir a la pestaÃ±a â€œModelsâ€.
- Abrir el modelo TelcoChurn_XGB.
- Verificar que la versiÃ³n correspondiente tenga el alias champion.
- Todo esto se realiza SIEMPRE en el MLflow remoto de DagsHub; Dagster nunca consulta el mlruns local.

### Otra opciÃ³n para ver el run champiion del experimento:

**Documentalmente**
- Dagster genera un documento .json de metadata donde podemos ver la infromaciÃ³n del champion:
dvc_prueba\tp_grupal_dagster\artifacts\champion_metadata.json

**Desde el Anaconda prompt3 del servidor de dagster:**
- El champion sensor automatizado, genera una lectura cada 30 segundos en busqueda de los runs actuales para detectar el nuevo champion, en ese mismo prompt3 podemos acceder rapidamente al run_id champion con el mejor f1. 

## ğŸ§ª Experimentos y Modelo CampeÃ³n

- El mejor modelo actual proviene del experimento telco_churn_tune_xgb.
- Dagster detectÃ³ automÃ¡ticamente el siguiente champion registrado en artifacts/champion_metadata.json:

| Atributo          | Valor                                      |
|-------------------|--------------------------------------------|
| Experimento       | `telco_churn_tune_xgb`                     |
| Modelo            | `TelcoChurn_XGB`                           |
| Run ID            | `53e572e30c7a46a49764166eb55a7302`         |
| MÃ©trica principal | `F1`                                       |
| Valor F1          | â‰ˆ **0.603** (`0.6028037383â€¦`)              |

-Este modelo es el que queda marcado con el alias champion en el Model Registry de MLflow.

## ğŸ“ˆ Reproducibilidad y CI/CD

**Comandos Ãºtiles DVC:**
- `dvc repro data_prep`
- `dvc dag`
- `dvc status`

### Workflows de GitHub Actions

**1) CI (`.github/workflows/ci.yml`)**

- Se ejecuta en cada `push` y `pull_request` a la rama `main`.
- Pasos principales:
  - Checkout del repositorio.
  - ConfiguraciÃ³n de Python 3.11.
  - InstalaciÃ³n de dependencias desde `requirements.txt`.
  - `dvc pull` usando los secrets de DagsHub (si falta algo en el remoto, el job no falla).
  - Chequeo de sintaxis del cÃ³digo:

    ```bash
    python scripts/base_scripts_runs.py
    ```

  - VerificaciÃ³n del `MLFLOW_TRACKING_URI`
    (imprime el valor de la variable de entorno y el `mlflow.get_tracking_uri()`).

**2) CD â€“ Retrain model and push to Dagshub (`.github/workflows/cd_retrain.yml`)**

- Se dispara manualmente desde la pestaÃ±a **Actions** (`workflow_dispatch`).
- Usa secrets del repo:
  - `DAGSHUB_USERNAME`, `DAGSHUB_TOKEN`
  - `MLFLOW_TRACKING_URI` (URL del tracking remoto en DagsHub).

- Pasos principales:
  - Checkout del repositorio.
  - ConfiguraciÃ³n de Python 3.11.
  - InstalaciÃ³n de dependencias.
  - ConfiguraciÃ³n del remoto DVC apuntando a DagsHub y `dvc pull` (mejor esfuerzo).
  - **Sanity check MLflow**: crea el experimento `ci_cd_sanity` y el run
    `gh_actions_smoke` en el MLflow remoto para confirmar credenciales/URI.
  - Ejecuta:

    ```bash
    python scripts/base_scripts_runs.py
    ```

    para reentrenar el modelo y loguear nuevos runs en el experimento
    `telco_churn_tune_xgb`, ademÃ¡s de actualizar el modelo `TelcoChurn_XGB`
    y el alias `champion` en el Model Registry de DagsHub.
  - `dvc push` para subir datos/artefactos versionados al remoto de DagsHub
    (si falla, el workflow no se rompe).

## ğŸ§© Visualizaciones

**El flujo genera:**
- Curvas ROC y PR en la carpeta reports/.
- Tabla de mÃ©tricas consolidada desde MLflow en la UI de DagsHub.
- Imagen comparativa de F1 por modelo (reports/f1_bench_dagster.png).
- Asset de champion actualizado (artifacts/champion_metadata.json).

## ğŸ“Œ Resultados Finales

**Modelo en â€œproducciÃ³nâ€ (Champion actual):**
- Algoritmo: TelcoChurn_XGB (clasificador XGBoost para churn)
- Experimento: telco_churn_tune_xgb
- F1-score â‰ˆ 0.603
- Registrado automÃ¡ticamente en el MLflow remoto de DagsHub como alias champion
- El sistema Dagster + MLflow permite mantener actualizado este modelo sin intervenciÃ³n manual, garantizando trazabilidad total del pipeline.

## ğŸš€ Deployment

- La estrategia de deployment propuesta (API REST, batch, monitoreo y reentrenamiento) se documenta en:
**DEPLOYMENT.md**

## ğŸ‘¤ Autores

- Nadia Soledad CasÃ¡
- Karla Silva

- Curso: Laboratorio de MinerÃ­a de Datos â€“ ISTEA
- AÃ±o: 2025


"Nota: esta secciÃ³n fue editada desde la rama demo_cliente para la demo."